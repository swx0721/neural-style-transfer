# src/train_model.py (MindSpore Version - FINAL - åŒ…å« StyleGenerator)

import mindspore as ms
# ğŸŒŸ å…³é”®ä¿®æ”¹ 1ï¼šå¯¼å…¥ MindSpore æƒé‡åŠ è½½æ‰€éœ€å‡½æ•°
from mindspore import nn, ops, load_checkpoint, load_param_into_net
from mindspore.common.initializer import XavierUniform, Constant
from mindspore.common.parameter import Parameter
import mindspore.numpy as ms_np
import os # ğŸŒŸ å…³é”®ä¿®æ”¹ 2ï¼šå¯¼å…¥ os åº“ç”¨äºè·¯å¾„æ£€æŸ¥

# --- 1. VGG19 Feature Extractor ---
class VGG19(nn.Cell):
    """VGG19 for feature extraction (partial implementation for NST layers)."""
    # ğŸŒŸ å…³é”®ä¿®æ”¹ 3ï¼šæ·»åŠ  checkpoint_path å‚æ•°
    def __init__(self, requires_grad=False, checkpoint_path=None):
        super(VGG19, self).__init__()
        
        # VGG19 blocks (only necessary layers for style/content features)
        # Block 1
        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu1_1 = nn.ReLU()
        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu1_2 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='valid')

        # Block 2
        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu2_1 = nn.ReLU()
        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu2_2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='valid')

        # Block 3
        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu3_1 = nn.ReLU()
        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu3_2 = nn.ReLU()
        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu3_3 = nn.ReLU()
        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu3_4 = nn.ReLU()
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='valid')

        # Block 4
        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu4_1 = nn.ReLU()
        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu4_2 = nn.ReLU()
        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu4_3 = nn.ReLU()
        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu4_4 = nn.ReLU()
        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='valid')

        # Block 5
        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu5_1 = nn.ReLU()
        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu5_2 = nn.ReLU()
        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu5_3 = nn.ReLU()
        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1, pad_mode='pad', has_bias=True)
        self.relu5_4 = nn.ReLU()
        self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='valid')
        
        # ----------------------------------------------------
        # ğŸŒŸ å…³é”®ä¿®æ”¹ 4ï¼šVGG19 æƒé‡åŠ è½½é€»è¾‘
        # ----------------------------------------------------
        if checkpoint_path and os.path.exists(checkpoint_path):
            print(f"[VGG19] Loading weights from: {checkpoint_path}")
            try:
                param_dict = load_checkpoint(checkpoint_path)
                load_param_into_net(self, param_dict)
                print("[VGG19] VGG19 Weights loaded successfully.")
            except Exception as e:
                # å¦‚æœ MindSpore ç‰ˆæœ¬ä¸åŒ¹é…æˆ–æƒé‡æ–‡ä»¶ç»“æ„å¼‚å¸¸ï¼Œä¼šåœ¨æ­¤å¤„æŠ¥é”™
                print(f"[VGG19] ERROR loading weights: {e}")
                print("[VGG19] Proceeding with randomly initialized weights (EXPECT FAILURE in forward pass).")
        else:
            print("[VGG19] WARNING: Checkpoint path not provided or file not found. Using random weights.")
            
        if not requires_grad:
            for param in self.get_parameters():
                param.requires_grad = False

    def construct(self, x):
        features = {}
        
        # Block 1
        x = self.relu1_1(self.conv1_1(x))
        features['relu1_1'] = x
        x = self.relu1_2(self.conv1_2(x))
        x = self.maxpool1(x)

        # Block 2
        x = self.relu2_1(self.conv2_1(x))
        features['relu2_1'] = x
        x = self.relu2_2(self.conv2_2(x))
        x = self.maxpool2(x)

        # Block 3
        x = self.relu3_1(self.conv3_1(x))
        features['relu3_1'] = x
        x = self.relu3_2(self.conv3_2(x))
        x = self.relu3_3(self.conv3_3(x))
        x = self.relu3_4(self.conv3_4(x))
        x = self.maxpool3(x)

        # Block 4
        x = self.relu4_1(self.conv4_1(x))
        features['relu4_1'] = x
        x = self.relu4_2(self.conv4_2(x))
        x = self.relu4_3(self.conv4_3(x))
        x = self.relu4_4(self.conv4_4(x))
        x = self.maxpool4(x)

        # Block 5
        x = self.relu5_1(self.conv5_1(x))
        features['relu5_1'] = x
        x = self.relu5_2(self.conv5_2(x))
        x = self.relu5_3(self.conv5_3(x))
        x = self.relu5_4(self.conv5_4(x))
        # maxpool5 åçš„ç‰¹å¾é€šå¸¸ä¸ç”¨äºé£æ ¼è¿ç§»
        
        return features

# --- 2. Style Transfer Network Components (StyleGenerator çš„è¾…åŠ©ç±») ---
# ... (ConvLayer, ResidualBlock, ConvTransposeLayer ç±»çš„ä»£ç ä¿æŒä¸å˜) ...

# å‡è®¾è¿™éƒ¨åˆ† StyleGenerator ä»£ç åœ¨åŸæ–‡ä»¶ä¸­å­˜åœ¨
class ConvLayer(nn.Cell):
    def __init__(self, in_channels, out_channels, kernel_size, stride):
        super(ConvLayer, self).__init__()
        self.reflect_pad = nn.Pad(paddings=((0, 0), (0, 0), (kernel_size//2, kernel_size//2), (kernel_size//2, kernel_size//2)), mode="REFLECT")
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0, pad_mode='valid', has_bias=True, weight_init=XavierUniform())
        self.instance_norm = nn.GroupNorm(num_groups=out_channels, num_channels=out_channels) # MindSpore ä½¿ç”¨ GroupNorm ä»£æ›¿ InstanceNorm
        self.relu = nn.ReLU()

    def construct(self, x):
        x = self.reflect_pad(x)
        x = self.conv(x)
        x = self.instance_norm(x)
        x = self.relu(x)
        return x

class ResidualBlock(nn.Cell):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        # ç¡®ä¿å·ç§¯æ ¸å¤§å°ä¸º 3, padding=1ï¼Œä¿æŒå°ºå¯¸ä¸å˜
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, pad_mode='pad', has_bias=True, weight_init=XavierUniform())
        self.in1 = nn.GroupNorm(num_groups=channels, num_channels=channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, pad_mode='pad', has_bias=True, weight_init=XavierUniform())
        self.in2 = nn.GroupNorm(num_groups=channels, num_channels=channels)
        self.relu = nn.ReLU()

    def construct(self, x):
        identity = x
        
        out = self.relu(self.in1(self.conv1(x)))
        out = self.in2(self.conv2(out))
        
        out += identity # æ®‹å·®è¿æ¥
        return out

class ConvTransposeLayer(nn.Cell):
    """ç”¨äºä¸Šé‡‡æ ·çš„åå·ç§¯å±‚ï¼ˆConvTranspose2dï¼‰"""
    def __init__(self, in_channels, out_channels, kernel_size, stride):
        super(ConvTransposeLayer, self).__init__()
        
        # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šå½“ pad_mode='same' æ—¶ï¼Œå¿…é¡»è®¾ç½® padding=0 (æˆ–çœç•¥)
        # MindSpore ä¼šè‡ªåŠ¨è®¡ç®—æ‰€éœ€å¡«å……ã€‚
        self.conv_transpose = nn.Conv2dTranspose(
            in_channels, 
            out_channels, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=0,                 # å¿…é¡»è®¾ä¸º 0
            pad_mode='same',           # ä¿æŒ 'same' æ¨¡å¼ï¼Œè®© MindSpore è‡ªåŠ¨å¤„ç†å¡«å……
            has_bias=True, 
            weight_init=XavierUniform()
        )
        self.instance_norm = nn.GroupNorm(num_groups=out_channels, num_channels=out_channels)
        self.relu = nn.ReLU()
    
    def construct(self, x):
        x = self.conv_transpose(x)
        x = self.instance_norm(x)
        x = self.relu(x)
        return x

# --- 3. StyleGenerator (å¿«é€Ÿé£æ ¼è¿ç§»ç½‘ç»œ) ---
class StyleGenerator(nn.Cell):
    def __init__(self):
        super(StyleGenerator, self).__init__()
        
        # ç¼–ç å™¨éƒ¨åˆ† (ä¸‹é‡‡æ ·)
        self.encode1 = ConvLayer(3, 32, kernel_size=9, stride=1)
        self.encode2 = ConvLayer(32, 64, kernel_size=3, stride=2)
        self.encode3 = ConvLayer(64, 128, kernel_size=3, stride=2)
        
        # æ®‹å·®å—éƒ¨åˆ†
        self.res_blocks = nn.SequentialCell([
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128)
        ])
        
        # è§£ç å™¨éƒ¨åˆ† (ä¸Šé‡‡æ ·)
        self.decode1 = ConvTransposeLayer(128, 64, kernel_size=3, stride=2)
        self.decode2 = ConvTransposeLayer(64, 32, kernel_size=3, stride=2)
        
        # è¾“å‡ºå±‚ (ä¸å¸¦ InstanceNorm å’Œ ReLU)
        self.output_padding = nn.Pad(paddings=((0, 0), (0, 0), (9//2, 9//2), (9//2, 9//2)), mode="REFLECT")
        self.conv_out = nn.Conv2d(32, 3, kernel_size=9, stride=1, padding=0, pad_mode='valid', has_bias=True)
        
        # MindSpore çš„ tanh æ¿€æ´»å‡½æ•°
        self.tanh = ops.Tanh()


    def construct(self, x):
        # ç¼–ç 
        x = self.encode1(x)
        x = self.encode2(x)
        x = self.encode3(x)
        
        # æ®‹å·®
        x = self.res_blocks(x)
        
        # è§£ç 
        x = self.decode1(x)
        x = self.decode2(x)
        
        # è¾“å‡ºå±‚
        x = self.output_padding(x)
        x = self.conv_out(x)
        
        # å°†è¾“å‡ºé™åˆ¶åœ¨ -1 åˆ° 1 é™„è¿‘ï¼Œä¸ VGG19 çš„å½’ä¸€åŒ–èŒƒå›´ä¿æŒä¸€è‡´
        return x # å®é™…ä¸Šä¸éœ€è¦ tanhï¼Œå› ä¸ºè¾“å…¥å›¾åƒæ˜¯åœ¨ MindSpore å½’ä¸€åŒ–åçš„èŒƒå›´

# --- 4. Gram Matrix and Total Loss Net ---

class GramMatrix(nn.Cell):
    """Compute the Gram matrix of a feature map."""
    def __init__(self):
        super(GramMatrix, self).__init__()
        self.reshape = ops.Reshape()
        # ğŸŒŸ å…³é”®ä¿®æ­£ 1ï¼šæ”¹ä¸º ops.BatchMatMul
        self.matmul = ops.BatchMatMul() 
        
        # ğŸŒŸ å…³é”®ä¿®æ­£ 2ï¼šä½¿ç”¨æ›´ç¨³å®šçš„ ops.Transpose 
        # (transpose_a=False, transpose_b=True æ„å‘³ç€è®¡ç®— A @ B^T)
        self.transpose_op = ops.Transpose()
        
        self.shape_op = ops.Shape()
        self.float_cast = ops.Cast()

    def construct(self, x):
        # x is (B, C, H, W)
        
        # 1. è·å–å½¢çŠ¶
        shape = self.shape_op(x)
        B, C, H, W = shape[0], shape[1], shape[2], shape[3] 
        
        # 2. Flatten H and W dimensions: (B, C, H*W)
        # æ­¤æ—¶ features æ˜¯ A
        features = self.reshape(x, (B, C, H * W)) 
        
        # 3. è®¡ç®— features_T (B, H*W, C)ï¼Œæ­¤æ—¶ features_T æ˜¯ B^T
        # B^T = Transpose(A) = Transpose(B, (0, 2, 1))
        features_T = self.transpose_op(features, (0, 2, 1))
        
        # 4. MatMul: (B, C, H*W) x (B, H*W, C) -> (B, C, C)
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨ ops.BatchMatMulï¼Œå®ƒçš„è¾“å…¥è¦æ±‚æ˜¯ (B, M, K) x (B, K, N)
        # è¿™é‡Œæ˜¯ (B, C, H*W) x (B, H*W, C)ï¼Œæ‰€ä»¥ M=C, K=H*W, N=C
        gram = self.matmul(features, features_T) 
        
        # 5. Normalization
        norm_factor = self.float_cast(ms.Tensor(C * H * W), ms.float32)
        gram = gram / norm_factor
        
        return gram


# --- Loss Function (Combined Content and Style Loss) ---

class NSTLoss(nn.Cell):
    """Calculates the total loss for Neural Style Transfer."""
    # ç”¨äºä¼ ç»Ÿé£æ ¼è¿ç§»ï¼ˆå›¾åƒè¿­ä»£ä¼˜åŒ–ï¼‰
    def __init__(self, style_features, content_features, style_weights, content_weight, style_weight):
        super(NSTLoss, self).__init__()
        self.style_features = style_features
        self.content_features = content_features
        self.style_weights = style_weights
        self.content_weight = content_weight
        self.style_weight = style_weight
        self.gram = GramMatrix()
        # MindSpore çš„ MSELoss é»˜è®¤æ˜¯ meanï¼Œè¿™é‡Œæ”¹ä¸º sum ä»¥åŒ¹é…åŸå§‹é€»è¾‘æˆ–ç®€å•ç›¸åŠ 
        self.mean_square_error = nn.MSELoss(reduction='sum') 
        
        # é»˜è®¤åªåœ¨ relu4_1 ä¸Šè®¡ç®—å†…å®¹æŸå¤±
        self.content_target = content_features['relu4_1']
        
    def construct(self, generated_features):
        # generated_features æ˜¯ç”± GradWrap ä¸­çš„ VGG19 è°ƒç”¨çš„ç»“æœï¼ˆç‰¹å¾å­—å…¸ï¼‰
        
        # 1. Content Loss (é€šå¸¸åªåœ¨ relu4_1 ä¸Šè®¡ç®—)
        content_loss_value = self.mean_square_error(generated_features['relu4_1'], self.content_target)
        
        # 2. Style Loss
        style_loss_value = ms.Tensor(0.0, ms.float32)
        
        # éå†æ‰€æœ‰é£æ ¼å±‚
        for name, weight in self.style_weights.items():
            # æå– Generated å›¾åƒçš„ Gram çŸ©é˜µ
            # è¾“å…¥æ˜¯ 4D ç‰¹å¾å›¾ (B, C, H, W)ï¼Œè¾“å‡ºæ˜¯ 2D Gram çŸ©é˜µ (B, C, C) æˆ– (C, C)
            generated_gram = self.gram(generated_features[name]) 
            
            # æå– Style ç›®æ ‡ Gram çŸ©é˜µ
            # ğŸŒŸ å…³é”®ä¿®æ­£ï¼šself.style_features[name] å·²ç»æ˜¯é¢„è®¡ç®—å¥½çš„ Gram çŸ©é˜µ (2D)ï¼Œç›´æ¥ä½¿ç”¨ã€‚
            style_gram = self.style_features[name]
            
            # è®¡ç®—è¯¥å±‚çš„é£æ ¼æŸå¤±
            layer_style_loss = self.mean_square_error(generated_gram, style_gram)
            
            # ç´¯åŠ åŠ æƒæŸå¤±
            style_loss_value += layer_style_loss * weight
            
        # 3. Total Loss
        total_loss = self.content_weight * content_loss_value + self.style_weight * style_loss_value
        
        return total_loss