# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
The types of arguments in Map must be consistent, but the types of arguments are inconsistent.
There are 5 inputs of `map`, corresponding type info:
In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 38~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                                      ^
.
The type of the second argument in Map is: Tensor[Float32].
The type of the third argument in Map is: Tuple[Ref[Tensor[Float32]]].
The type of the 4th argument in Map is: Tuple[Ref[Tensor[Float32]]].
The type of the 5th argument in Map is: Tuple[Ref[Tensor[Float32]]].

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\ccsrc\frontend\operator\composite\map.cc:237 mindspore::prim::Map::Make

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:840~841, 8~64
        if not self.use_offload:
# 1 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:841, 12~64
            gradients = self.gradients_centralization(gradients)
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:852, 15~98
        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)
               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:744~829, 8~65
        if self.use_offload:
# 4 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65
            if self.use_dist_optimizer:
# 5 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65
                if self.is_group_lr:
# 6 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65
                    if self.use_lazy:
# 7 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65
                        if self.use_amsgrad:
# 8 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 28~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                            ^
# 9 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65
                        if self.use_amsgrad:
# 10 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65
                    if self.use_lazy:
# 11 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65
                if self.is_group_lr:
# 12 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65
            if self.use_dist_optimizer:
# 13 In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 38~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                                      ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_optim_adam_Adam_construct_4
# Total subgraphs: 0

# Attrs:
skip_auto_parallel_compile: 1

# Total params: 8
# Params:
%para1_gradients: <null>
%para2_global_step: <Ref[Tensor[Int32]], (1), ref_key=global_step, is_parameter>  :  has_default
%para3_beta1_power: <Ref[Tensor[Float32]], (), ref_key=beta1_power, is_parameter>  :  has_default
%para4_beta2_power: <Ref[Tensor[Float32]], (), ref_key=beta2_power, is_parameter>  :  has_default
%para5_generated_image: <Ref[Tensor[Float32]], (1, 3, 734, 979), ref_key=generated_image, is_parameter>  :  has_default
%para6_moment1.generated_image: <Ref[Tensor[Float32]], (1, 3, 734, 979), ref_key=moment1.generated_image, is_parameter>  :  has_default
%para7_moment2.generated_image: <Ref[Tensor[Float32]], (1, 3, 734, 979), ref_key=moment2.generated_image, is_parameter>  :  has_default
%para8_learning_rate: <Ref[Tensor[Float32]], (), ref_key=learning_rate, is_parameter>  :  has_default

subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_4 : 00000215A5ACCD10
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_4() {
  %0(CNode_18) = resolve(NameSpace[Entry: 'mindspore.nn.optim.adam.Adam.construct'], mindspore.nn.optim.adam.Adam.construct)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)

#------------------------> 0
  %1(CNode_19) = %0(%para1_gradients)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:840~841, 8~64/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_4:CNode_18{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Entry: 'mindspore.nn.optim.adam.Adam.construct', [2]: ValueNode<Symbol> mindspore.nn.optim.adam.Adam.construct}
#   2: @mindspore_nn_optim_adam_Adam_construct_4:CNode_19{[0]: CNode_18, [1]: param_gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_4:CNode_20{[0]: ValueNode<Primitive> Return, [1]: CNode_19}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_4 : 00000215A5ACF400
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_4(%para0_gradients) {

#------------------------> 1
  %0(CNode_21) = call @mindspore_nn_optim_adam_Adam_construct_5()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:840~841, 8~64/        if not self.use_offload:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:840~841, 8~64/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_4:params{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> _parameters}
#   2: @mindspore_nn_optim_adam_Adam_construct_4:moment1{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> moment1}
#   3: @mindspore_nn_optim_adam_Adam_construct_4:moment2{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> moment2}
#   4: @mindspore_nn_optim_adam_Adam_construct_4:CNode_22{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> flatten_gradients}
#   5: @mindspore_nn_optim_adam_Adam_construct_4:CNode_23{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   7: @mindspore_nn_optim_adam_Adam_construct_4:gradients{[0]: CNode_22, [1]: param_gradients}
#   8: @mindspore_nn_optim_adam_Adam_construct_4:CNode_24{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> decay_weight}
#   9: @mindspore_nn_optim_adam_Adam_construct_4:CNode_25{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  11: @mindspore_nn_optim_adam_Adam_construct_4:gradients{[0]: CNode_24, [1]: gradients}
#  12: @mindspore_nn_optim_adam_Adam_construct_4:CNode_21{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_5}
#  13: @mindspore_nn_optim_adam_Adam_construct_4:CNode_20{[0]: ValueNode<Primitive> Return, [1]: CNode_21}
#  14: @mindspore_nn_optim_adam_Adam_construct_4:CNode_26{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> gradients_centralization}
#  15: @mindspore_nn_optim_adam_Adam_construct_4:CNode_27{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> scale_grad}
#  16: @mindspore_nn_optim_adam_Adam_construct_4:CNode_28{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> _grad_sparse_indices_deduplicate}
#  17: @mindspore_nn_optim_adam_Adam_construct_4:CNode_29{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> get_lr}
#  18: @mindspore_nn_optim_adam_Adam_construct_4:CNode_30{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> global_step}
#  19: @mindspore_nn_optim_adam_Adam_construct_4:CNode_31{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> global_step_increase_tensor}
#  20: @mindspore_nn_optim_adam_Adam_construct_4:CNode_32{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta1_power}
#  21: @mindspore_nn_optim_adam_Adam_construct_4:CNode_33{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta1}
#  22: @mindspore_nn_optim_adam_Adam_construct_4:CNode_34{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta2_power}
#  23: @mindspore_nn_optim_adam_Adam_construct_4:CNode_35{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta2}
#  24: @mindspore_nn_optim_adam_Adam_construct_4:CNode_36{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> _apply_adam}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_5 : 00000215A5ACBC60
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_5 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_4]() {

#------------------------> 2
  %0(CNode_37) = call @mindspore_nn_optim_adam_Adam_construct_6()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:841, 12~64/            gradients = self.gradients_centralization(gradients)/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:841, 12~64/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_5:CNode_38{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   3: @mindspore_nn_optim_adam_Adam_construct_5:gradients{[0]: CNode_26, [1]: gradients}
#   4: @mindspore_nn_optim_adam_Adam_construct_5:CNode_37{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_6}
#   5: @mindspore_nn_optim_adam_Adam_construct_5:CNode_39{[0]: ValueNode<Primitive> Return, [1]: CNode_37}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_6 : 00000215A5ACC1F0
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_6 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_5]() {
  %0(CNode_40) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], assignadd)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:845, 8~22/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %1(CNode_30) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], global_step)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Int32]], (1)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:845, 23~39/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_31) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], global_step_increase_tensor)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Int32], (1)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:845, 41~73/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %3(CNode_41) = %0(%1, %2)
      : (<Ref[Tensor[Int32]], (1)>, <Tensor[Int32], (1)>) -> (<Ref[Tensor[Int32]], (1)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:845, 8~74/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %4(CNode_32) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta1_power)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:847, 22~38/        beta1_power = self.beta1_power * self.beta1/
  %5(CNode_42) = resolve(NameSpace[Ast: 'Namespace:mindspore._extends.parse.trope'], mul)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:847, 22~51/        beta1_power = self.beta1_power * self.beta1/
  %6(CNode_33) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:847, 41~51/        beta1_power = self.beta1_power * self.beta1/
  %7(beta1_power) = %5(%4, %6)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:847, 22~51/        beta1_power = self.beta1_power * self.beta1/
  %8(CNode_43) = call @assign_44(%4, %7)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:848, 8~38/        self.beta1_power = beta1_power/
  %9(CNode_34) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta2_power)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:849, 22~38/        beta2_power = self.beta2_power * self.beta2/
  %10(CNode_45) = resolve(NameSpace[Ast: 'Namespace:mindspore._extends.parse.trope'], mul)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:849, 22~51/        beta2_power = self.beta2_power * self.beta2/
  %11(CNode_35) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:849, 41~51/        beta2_power = self.beta2_power * self.beta2/
  %12(beta2_power) = %10(%9, %11)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:849, 22~51/        beta2_power = self.beta2_power * self.beta2/
  %13(CNode_46) = call @assign_44(%9, %12)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:850, 8~38/        self.beta2_power = beta2_power/
  %14(CNode_47) = MakeTuple(%3, %8, %13)
      : (<Ref[Tensor[Int32]], (1)>, <Ref[Tensor[Float32]], ()>, <Ref[Tensor[Float32]], ()>) -> (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
  %15(CNode_48) = StopGradient(%14)
      : (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>) -> (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:833~852, 4~98/    @jit/
  %16(CNode_36) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], _apply_adam)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:852, 15~31/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %17(params) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], _parameters)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:835, 17~33/        params = self._parameters/
  %18(moment1) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], moment1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:836, 18~30/        moment1 = self.moment1/
  %19(moment2) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], moment2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:837, 18~30/        moment2 = self.moment2/
  %20(CNode_29) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], get_lr)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:844, 13~24/        lr = self.get_lr()/
  %21(lr) = %20()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:844, 13~26/        lr = self.get_lr()/
  %22(CNode_28) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], _grad_sparse_indices_deduplicate)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:843, 20~57/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %23(CNode_27) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], scale_grad)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:842, 20~35/        gradients = self.scale_grad(gradients)/
  %24(CNode_26) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], gradients_centralization)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:841, 24~53/            gradients = self.gradients_centralization(gradients)/
  %25(CNode_24) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], decay_weight)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:839, 20~37/        gradients = self.decay_weight(gradients)/
  %26(CNode_22) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], flatten_gradients)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:838, 20~42/        gradients = self.flatten_gradients(gradients)/
  %27(gradients) = %26(%para0_gradients)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<Tensor[Float32], (1, 3, 734, 979)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:838, 20~53/        gradients = self.flatten_gradients(gradients)/
  %28(gradients) = %25(%27)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<Tensor[Float32], (1, 3, 734, 979)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:839, 20~48/        gradients = self.decay_weight(gradients)/
  %29(gradients) = %24(%28)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<Tensor[Float32], (1, 3, 734, 979)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:841, 24~64/            gradients = self.gradients_centralization(gradients)/
  %30(gradients) = %23(%29)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<Tensor[Float32], (1, 3, 734, 979)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:842, 20~46/        gradients = self.scale_grad(gradients)/
  %31(gradients) = %22(%30)
      : (<Tensor[Float32], (1, 3, 734, 979)>) -> (<Tensor[Float32], (1, 3, 734, 979)>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:843, 20~68/        gradients = self._grad_sparse_indices_deduplicate(gradients)/

#------------------------> 3
  %32(CNode_49) = %16(%17, %7, %12, %18, %19, %21, %31)
      : (<Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>, <Ref[Tensor[Float32]], ()>, <Tensor[Float32], (1, 3, 734, 979)>) -> (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:852, 15~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %33(CNode_50) = Depend(%32, %15) primitive_attrs: {side_effect_propagate: I64(1)} cnode_attrs: {topo_sort_rhs_first: Bool(1)}
      : (<null>, <Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>) -> (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:852, 15~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%33)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:852, 8~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_6:CNode_51{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   3: @mindspore_nn_optim_adam_Adam_construct_6:gradients{[0]: CNode_27, [1]: gradients}
#   4: @mindspore_nn_optim_adam_Adam_construct_6:CNode_52{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   6: @mindspore_nn_optim_adam_Adam_construct_6:gradients{[0]: CNode_28, [1]: gradients}
#   7: @mindspore_nn_optim_adam_Adam_construct_6:lr{[0]: CNode_29}
#   8: @mindspore_nn_optim_adam_Adam_construct_6:CNode_40{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> assignadd}
#   9: @mindspore_nn_optim_adam_Adam_construct_6:CNode_53{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  11: @mindspore_nn_optim_adam_Adam_construct_6:CNode_41{[0]: CNode_40, [1]: CNode_30, [2]: CNode_31}
#  12: @mindspore_nn_optim_adam_Adam_construct_6:CNode_42{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Ast: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> mul}
#  13: @mindspore_nn_optim_adam_Adam_construct_6:beta1_power{[0]: CNode_42, [1]: CNode_32, [2]: CNode_33}
#  14: @mindspore_nn_optim_adam_Adam_construct_6:CNode_43{[0]: ValueNode<FuncGraph> assign_44, [1]: CNode_32, [2]: beta1_power}
#  15: @mindspore_nn_optim_adam_Adam_construct_6:CNode_45{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Ast: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> mul}
#  16: @mindspore_nn_optim_adam_Adam_construct_6:beta2_power{[0]: CNode_45, [1]: CNode_34, [2]: CNode_35}
#  17: @mindspore_nn_optim_adam_Adam_construct_6:CNode_46{[0]: ValueNode<FuncGraph> assign_44, [1]: CNode_34, [2]: beta2_power}
#  18: @mindspore_nn_optim_adam_Adam_construct_6:CNode_54{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  20: @mindspore_nn_optim_adam_Adam_construct_6:CNode_49{[0]: CNode_36, [1]: params, [2]: beta1_power, [3]: beta2_power, [4]: moment1, [5]: moment2, [6]: lr, [7]: gradients}
#  22: @mindspore_nn_optim_adam_Adam_construct_6:CNode_55{[0]: ValueNode<Primitive> Return, [1]: CNode_50}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_7 : 00000215A5E39430
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_7(%para0_params, %para0_beta1_power, %para0_beta2_power, %para0_moment1, %para0_moment2, %para0_lr, %para0_gradients) {

#------------------------> 4
  %0(CNode_56) = call @_apply_adam_8()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:744~829, 8~65/        if self.use_offload:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:744~829, 8~65/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_7:CNode_56{[0]: ValueNode<FuncGraph> _apply_adam_8}
#   2: @_apply_adam_7:CNode_57{[0]: ValueNode<Primitive> Return, [1]: CNode_56}
#   3: @_apply_adam_7:CNode_58{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> map_}
#   4: @_apply_adam_7:CNode_59{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.optim.adam', [2]: ValueNode<Symbol> F}
#   5: @_apply_adam_7:CNode_60{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.optim.adam', [2]: ValueNode<Symbol> _adam_opt}
#   6: @_apply_adam_7:CNode_61{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta1}
#   7: @_apply_adam_7:CNode_62{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> beta2}
#   8: @_apply_adam_7:CNode_63{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> eps}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_8 : 00000215A5E399C0
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_8 parent: [subgraph @_apply_adam_7]() {

#------------------------> 5
  %0(CNode_64) = call @_apply_adam_9()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65/            if self.use_dist_optimizer:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_8:CNode_64{[0]: ValueNode<FuncGraph> _apply_adam_9}
#   2: @_apply_adam_8:CNode_65{[0]: ValueNode<Primitive> Return, [1]: CNode_64}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_9 : 00000215A5E38910
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_9 parent: [subgraph @_apply_adam_7]() {

#------------------------> 6
  %0(CNode_66) = call @_apply_adam_10()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65/                if self.is_group_lr:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_9:CNode_66{[0]: ValueNode<FuncGraph> _apply_adam_10}
#   2: @_apply_adam_9:CNode_67{[0]: ValueNode<Primitive> Return, [1]: CNode_66}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_10 : 00000215A5E3B590
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_10 parent: [subgraph @_apply_adam_7]() {

#------------------------> 7
  %0(CNode_68) = call @_apply_adam_11()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65/                    if self.use_lazy:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_10:CNode_68{[0]: ValueNode<FuncGraph> _apply_adam_11}
#   2: @_apply_adam_10:CNode_69{[0]: ValueNode<Primitive> Return, [1]: CNode_68}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_11 : 00000215A5E38380
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_11 parent: [subgraph @_apply_adam_7]() {

#------------------------> 8
  %0(CNode_70) = call @_apply_adam_12()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65/                        if self.use_amsgrad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_11:CNode_70{[0]: ValueNode<FuncGraph> _apply_adam_12}
#   2: @_apply_adam_11:CNode_71{[0]: ValueNode<Primitive> Return, [1]: CNode_70}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_12 : 00000215A5E3B000
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_12 parent: [subgraph @_apply_adam_7]() {

#------------------------> 9
  %0(CNode_72) = call @_apply_adam_13()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 28~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 28~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
}
# Order:
#   1: @_apply_adam_12:CNode_73{[0]: ValueNode<Primitive> getattr, [1]: CNode_59, [2]: ValueNode<StringImm> partial}
#   2: @_apply_adam_12:CNode_74{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> opt}
#   3: @_apply_adam_12:CNode_75{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> sparse_opt}
#   4: @_apply_adam_12:CNode_76{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> use_locking}
#   5: @_apply_adam_12:CNode_77{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> use_nesterov}
#   6: @_apply_adam_12:CNode_78{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>', [2]: ValueNode<Symbol> _is_device}
#   7: @_apply_adam_12:CNode_79{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   9: @_apply_adam_12:CNode_80{[0]: CNode_73, [1]: CNode_60, [2]: CNode_74, [3]: CNode_75, [4]: CNode_76, [5]: CNode_77, [6]: CNode_78, [7]: param_beta1_power, [8]: param_beta2_power, [9]: CNode_61, [10]: CNode_62, [11]: CNode_63, [12]: param_lr}
#  10: @_apply_adam_12:CNode_81{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  12: @_apply_adam_12:success{[0]: CNode_58, [1]: CNode_80, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2}
#  13: @_apply_adam_12:CNode_72{[0]: ValueNode<FuncGraph> _apply_adam_13}
#  14: @_apply_adam_12:CNode_82{[0]: ValueNode<Primitive> Return, [1]: CNode_72}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_13 : 00000215A5E37DF0
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_13 parent: [subgraph @_apply_adam_12]() {

#------------------------> 10
  %0(CNode_83) = call @_apply_adam_14()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65/                        if self.use_amsgrad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:818~829, 24~65/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_13:CNode_83{[0]: ValueNode<FuncGraph> _apply_adam_14}
#   2: @_apply_adam_13:CNode_84{[0]: ValueNode<Primitive> Return, [1]: CNode_83}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_14 : 00000215A5E3A4E0
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_14 parent: [subgraph @_apply_adam_12]() {

#------------------------> 11
  %0(CNode_85) = call @_apply_adam_15()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65/                    if self.use_lazy:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:812~829, 20~65/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_14:CNode_85{[0]: ValueNode<FuncGraph> _apply_adam_15}
#   2: @_apply_adam_14:CNode_86{[0]: ValueNode<Primitive> Return, [1]: CNode_85}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_15 : 00000215A5E3AA70
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_15 parent: [subgraph @_apply_adam_12]() {

#------------------------> 12
  %0(CNode_87) = call @_apply_adam_16()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65/                if self.is_group_lr:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:792~829, 16~65/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_15:CNode_87{[0]: ValueNode<FuncGraph> _apply_adam_16}
#   2: @_apply_adam_15:CNode_88{[0]: ValueNode<Primitive> Return, [1]: CNode_87}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_16 : 00000215A5E39F50
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_16 parent: [subgraph @_apply_adam_12]() {

#------------------------> 13
  %0(CNode_89) = call @_apply_adam_17()
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65/            if self.use_dist_optimizer:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:753~829, 12~65/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_16:CNode_89{[0]: ValueNode<FuncGraph> _apply_adam_17}
#   2: @_apply_adam_16:CNode_90{[0]: ValueNode<Primitive> Return, [1]: CNode_89}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_17 : 00000215A5E38EA0
# In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:742~831, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_17 parent: [subgraph @_apply_adam_12]() {
  %0(CNode_58) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], map_)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 38~47/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %1(CNode_59) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.optim.adam'], F)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 48~49/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %2(CNode_73) = getattr(%1, "partial")
      : (<External, NoShape>, <String, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 48~57/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %3(CNode_60) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.optim.adam'], _adam_opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 58~67/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %4(CNode_74) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 69~77/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %5(CNode_75) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], sparse_opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825, 79~94/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %6(CNode_76) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], use_locking)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:826, 58~74/                                                          self.use_locking, self.use_nesterov,/
  %7(CNode_77) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], use_nesterov)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:826, 76~93/                                                          self.use_locking, self.use_nesterov,/
  %8(CNode_78) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], _is_device)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:827, 58~73/                                                          self._is_device, beta1_power, beta2_power,/
  %9(CNode_61) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:828, 58~68/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %10(CNode_62) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], beta2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:828, 70~80/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %11(CNode_63) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::2291823835424>'], eps)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:828, 82~90/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %12(CNode_80) = %2(%3, %4, %5, %6, %7, %8, $(@_apply_adam_7:para0_beta1_power), $(@_apply_adam_7:para0_beta2_power), %9, %10, %11, $(@_apply_adam_7:para0_lr))
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Bool, NoShape>, <Bool, NoShape>, <Bool, NoShape>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Ref[Tensor[Float32]], ()>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~828, 48~95/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/

#------------------------> 14
  %13(success) = %0(%12, $(@_apply_adam_7:para0_gradients), $(@_apply_adam_7:para0_params), $(@_apply_adam_7:para0_moment1), $(@_apply_adam_7:para0_moment2))
      : (<Func, NoShape>, <Tensor[Float32], (1, 3, 734, 979)>, <Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((1, 3, 734, 979))>) -> (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:825~829, 38~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  Return(%13)
      : (<null>)
      #scope: (Default)
      # In file D:\conda\envs\dachuang\lib\site-packages\mindspore\nn\optim\adam.py:831, 8~22/        return success/
}
# Order:
#   1: @_apply_adam_17:CNode_91{[0]: ValueNode<Primitive> Return, [1]: success}


# ===============================================================================================
# The total of function graphs in evaluation stack: 15/16 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
No more function graphs.

